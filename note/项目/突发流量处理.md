一、问题
1. 一个普通系统，突发10w并发，如何保证系统不雪崩
2. 服务层的处理：
2.1 服务扩容：服务应该是无状态的，支持水平扩容，最好是支持自动扩容
2.2 服务代码的处理：支持高并发，gin就是每个请求都开启一个协程处理；链路服务每个tcp链接都有三个协程读goroutine、包处理函数、写goroutine，可以用epoll优化
2.3 限流：限流是最坏的处理，如果并发流量是有限流量应该都要请求到服务层
3. 缓存层：mysql只能支持1k级别的并发，redis支持2w左右（当然redis官方说能支持10w），如果存在读写那支持的并发更少，解决方案是多级缓存：二级缓存、三级缓存
3.1 redis的节点扩容，每个redis节点支持2w的qps，如果5～7个master节点就可以支持10w了，但是这是流量均匀打到集群的情况，突发流量更多的是热点key，通常会流量不均
3.2 二级缓存：redis缓存+本地缓存
3.3 三级缓存：redis缓存+本地缓存+nginx缓存；nginx会缓存一些热点数据，但是因为nginx只能缓存少量数据命中率低，可以分成nginx分发层和nginx层

二、扩容
1. 扩容的前提是服务是无状态的，web服务一般都能做到，像tcp服务因为长连接的关系，每个服务都管理着部分链接，需要一个proxy层来确认数据包发向哪个服务，这样可以支持扩容，但是缩容的话需要运维进行摘流，服务上没有客户端tcp链接才可以缩容
2. 如果是手动水平扩容，是无法支持突发流量，需要自动扩容，k8s的自动扩容方案hpa, kpa ca, vpa
2.1 hpa:根据监控指标如cpu使用率、磁盘等，自动扩缩容pod数量
2.1.1 在k8s hpav1版本，只支持cpu使用率的自动扩缩容，根据qps做出监控指标才更能处理并发流量；在k8s hpav2支持基于内存和自定义指标
2.1.2 如何采集监控指标：基于prometheus，sdk会自动监控系统的一些指标，也可以自定义metric参数；基于pull模型prometheus会定时拉取监控数据
2.1.3 hpa如何实现的自动扩缩容？分为三步
2.1.3.1 获得被hpa管理的所有pod的监控指标
2.1.3.2 计算出预期工作负载的副本数
2.1.3.3 修改管理pod的replicas的数量，ReplicaSet控制器来控制pod数量
2.1.3.4 这里有些问题：默认30s检查一次指标；为了避免频繁的扩缩容，在5min内没有重新扩缩容的情况下才会进行扩缩容；对于突然流量实际上也支持不了；想到的解决方案是hpa controller提高扫描频率，扩容快不需要5分钟，缩容可以5分钟；有severless场景，缩容到0然后冷启动，hpa默认不支持
2.2 kpa：基于请求数扩缩容，并且设置扩缩容边界自动扩缩容
2.3 vpa：pod垂直扩缩容，基于pod的资源使用情况，为集群设置资源占用限制，让集群将pod调度到资源足够的节点；可以缩小过度请求资源的容器也可以根据情况提升资源不足的容器
2.3.1 vpa和hpa不能共同使用
2.4 ca: 根据pod的资源请求自动添加或删除集群节点，用来弹性伸缩k8s集群

二、限流
1. 限流有四种算法：计数器、滑动窗口、漏桶、令牌桶
2. 使用sentinel go来做限流和熔断

三、二级缓存
1. 二级缓存指redis缓存+本地缓存，redis只能支持2w的qps，需要本地缓存热点数据
2. 本地缓存的优点：直接请求内存速度快；缺点：需要解决数据一致性的问题，只能存储少量数据，内存数据不能持久化
3. 数据一致性：本地缓存-redis数据-db数据的数据一致性，策略是本地缓存创建etcd watcher业务key，当数据修改时，保证mysql+redis的数据一致性，这里可以用锁，然后put修改业务key，本地缓存监听到后读redis数据重建本地缓存
4. 本地缓存尽量lfu、lru算法，提供命中率
5. 本地缓存尽量缓存热点数据，这里涉及redis热点key的检测
